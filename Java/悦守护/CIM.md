# CIM

## 系统架构
![img](https://camo.githubusercontent.com/16f644ac7e2ab8cf8b8784408b1c70baf15634f4/68747470733a2f2f7773312e73696e61696d672e636e2f6c617267652f303036744e6252776c793166796c646769697a68756a3331356f3072346e306b2e6a7067)







## 方案
### 通信方式
一般用服务器中转方式
#### P2P方式

P2P多见于局域网内聊天工具，典型的应用有:飞鸽传书、天网Maze(你懂的)等。这类软件在启动后一般做两件事情：
* 进行UDP广播:发送自己信息和接受同局域网内其他端信息；
* 开启TCP监听:等待其他端进行连接。
#### 服务器中转方式

几乎所有互联网IM产品都采用服务器中转这种方式进行消息传输，相对于P2P的方式，它有如下的优点:
* 能够支持更多P2P无法支持或支持不好的业务，如离线消息，群组，聊天室服务；
* 方便业务逻辑的拓展和新旧版本的兼容。

当然它也有自己的问题:服务器架构复杂，并发要求高。
### 网络通讯技术
一般用 基于TCP的长连接
IM主流网络通讯技术有两种:

* 基于TCP的长连接；
* 基于HTTP短连接PULL的方式。

后者常见于WEB IM系统(当然现在很多WEB IM都是基于WebSocket实现)，它的优点是实现简单，方便开发上手，问题是流量大，服务器负载较大，
消息及时性无法很好地保证，对大规模的用户量支持不够，比较适合小型的IM系统,如小网站的客户系统。
基于TCP长连接则能够更好地支持大批量用户，问题是客户端和服务器的实现比较复杂。当然也还有一些变种，如下行使用MQTT进行服务器通知/消息的下发，
上行使用HTTP短连接进行指令和消息的上传。这种方式能够保证下行消息/指令的及时性，但是在弱网络下上行慢的问题还是比较严重。早期的来往就是基于这种方式。

### 消息协议
采用ProtoBuf协议, 现在主流的IM都在用。微信、手机qq基本上都是用这
优点：非常小、非常快、非常简单，一条消息数据用Protobuf序列化后的大小是JSON的1/10、XML格式的1/20、是二进制序列化的1/10。
缺点：不能表示复杂的数据结构，但是对于IM来讲，已经足够。强烈推荐此协议。
灵活、高效：灵活（方便接口更新）、高效（效率经过google的优化，传输效率比普通的XML等高很多）；
易于使用：开发人员通过按照一定的语法定义结构化的消息格式，然后送给命令行工具，工具将自动生成相关的类，可以支持java、c++、python等语言环境。
通过将这些类包含在项目中，可以很轻松的调用相关方法来完成业务消息的序列化与反序列化工作。
语言支持：原生支持c++、java、python等多达10余种语言。
需要和其它系统做消息交换的，对消息大小很敏感的。那么protobuf适合了，它语言无关，消息空间相对xml和json等节省很多。
小数据的场合。如果你是大数据，用它并不适合。
项目语言是c++、java、python等，因为它们可以使用google的源生类库，序列化和反序列化的效率非常高。其它的语言需要第三方或者自己写，序列化和反序列化的效率不保证。

### 其他问题
#### 协议加密
般出于效率的考虑都会采用流式加密，如RC4。而前期协商过程则推荐使用RSA等非对称加密以增加破解难度。
#### 快速连接（即掉线重连机制）
对iOS APP而言，因为没有真后台的存在，APP每次启动基本都需要一次重连登录(短时间内切换除外)，所以如何快速重连、重登就非常重要。
常见优化思路如下:

本地缓存服务器IP并定期刷新。移动网络调优可以参考《iOS端移动网络调优的8条建议》；
合并部分请求。如加密和登录操作可以合并为同一个操作，这样就可以减少一次不必要的网络请求来回的时间；
简化登录后的同步请求，部分同步请求可以推迟到UI操作时进行，如群成员信息刷新。
#### 连接保持（即心跳机制）
一般APP实现连接保持的方式无非是采用应用层的心跳，通过心跳包的超时和其他条件(网络切换)来执行重连操作。那么问题来了:为什么要使用应用层心跳和如何设计应用层心跳。众所周知TCP协议是有KEEPALIVE这个设置选项，设置为KEEPALIVE后，客户端每隔N秒(默认是7200s)会向服务器发送一个发送心跳包。

但实际操作中我们更多的是使用应用层心跳。原因如下:

KEEPALIVE对服务器负载压力比较大(服务器大大是这么说的...)；
socks代理对KEEPALIVE并不支持；
部分复杂情况下KEEPALIVE会失效，如路由器挂掉，网线(移动端没有网线...)直接被拔除。

移动端在实际操作时为了节约流量和电量一般会在心跳包上做一些小优化：

精简心跳包，保证一个心跳包大小在10字节之内；
心跳包只在空闲时发送；
根据APP前后台状态调整心跳包间隔 (主要是安卓)。
#### 消息可达
见IM-Server部分
#### 文件上传优化

常见有下面这些优化思路:

* 将上传流程提前:音频提供边录边传。朋友圈的图片进行预上传，选择图片后用户一般会进行文本输入，在这段时间内后台就可以默默将选好的图片进行上传；
* 提供闪电上传的方式:服务器根据MD5进行文件去重；
* 优化和上传服务器的连接(参考快速连接)，提供连接重用的功能；
* **文件分块上传**:因为移动网络丢包严重，将文件分块上传可以使得一个分组包含合理数量的TCP包，使得重试概率下降，重试代价变小，更容易上传到服务器；
* 在分包的前提下支持上传的pipeline，避免不必要的网络等待时间；
* 支持断点续传。

## 连接器的设计
底层通信基于高性能的网络框架Netty4.0.
## 中间件的设计
消息发送使用RabbitMQ,MQ消息队列目前在中大型分布式系统实际应用中常用的使用场景主要有：异步处理、应用解耦、流量削峰和消息通讯四个场景。
ClientA发送消息到IM-Server后,IM-Server先将消息发送至MQ,然后在发送到ClientB。
在一个典型的IM即时通讯应用中，MQ消息队列可以用于：

1）用户的聊天消息离线存储环节：因为IM消息的发送属于高吞吐场景，直接操纵DB很容易就把DB搞挂了，所以离线消息在落地入库前，
    可以先扔到MQ消息队列中，再由单独部署的消费者来有节奏地存储到DB中；

2）用户的行为数据收集环节：因为用户的聊天消息和指令等，可以用于大数据分析，而且基于国家监管要求也是必须要存储一段时间的，
    所以此类数据的收集同样可以用于MQ消息队列，再由单独部署的消费者存储到DB中；

3）用户的操作日志收集环节：log这种数据价值不高，但关键时刻又非常有用，而且数据量又很大，要想存储起来难度很高，
    这时就轮到Linkedin公司开源的Kafka出场了；


## 逻辑服务器
登录注册、设备绑定、SOS、电子围栏、集赞等逻辑服务。
用户登录后将用户信息发送至状态服务器,由状态服务器将信息保存至Redis然后与IM-Server 建立TCP连接。
## (状态服务器)路由层

用于处理消息路由、消息转发、用户登录、用户下线以及一些运营工具（后台监控-->获取在线用户数等）。
### 用户登录
登录时先根据Redis数据判断是否重复登录。
### 登录成功
用户登录成功后 查询ZooKeeper中可用IM-Server 让当前用户与其中某个(一致性hash算法)IM-Server建立TCP长连接。
同时保存用户信息,登录状态,路由信息等到Redis.
### 用户下线
用户下线则删除这些数据

## IM-Server
用于接收 client 连接、消息透传、消息推送等功能。

底层通信基于高性能的网络框架Netty4.0.
其中文字消息直接发送,图片、语音等大文件先发送到HTTP服务器,在将文件的URL返回给发送方,这样转成文字后,发送方在将URL发送给接收方。
其中图片只显示缩略图,语音则显示图标,用户点击时才下载。


```java
-- 消息接收者ID
receiver_uid varchar(50), 
 
-- 消息的唯一指纹码（即消息ID），用于去重等场景，单机情况下此id可能是个自增值、分布式场景下可能是类似于UUID这样的东西
msg_id varchar(70), 
 
-- 消息发出时的时间戳（如果是个跨国IM，则此时间戳可能是GMT-0标准时间）       
send_time time, 
 
-- 消息发送者ID
sender_uid varchar(50), 
 
-- 消息类型（标识此条消息是：文本、图片还是语音留言等）
msg_type int, 
 
-- 消息内容（如果是图片或语音留言等类型，由此字段存放的可能是对应文件的存储地址或CDN的访问URL）
msg_content varchar(1024), 
```
### 1.如何保证消息不丢失
6个报文
**切记,一条消息的发送,包含上半场msg:R/A/N与下半场ack:R/A/N的6个报文**。
其中R-->Request  A-->ACK  N-->Notify
```java
1.msg:R-->ClientA 发送消息给IM-Server  
2.msg:A-->IM-Server收到消息给ClientA一个提醒。
3.msg:N-->IM-Server将消息发送给ClientB,提示你有一个消息来了。
4.ack:R-->ClientB收到消息后向IM-Server发送一个ack请求表示自己收到消息了
5.ack:A-->IM-Server 收到ClientB的ack请求后回一个ack:A,表示知道你收到消息了。
6.ack:N-->IM-Server 向ClientA发送一个ack:N包,通知ClientA,你的消息已经成功被ClientB接收了。
```
### 2.失败重传
ClientA发出了msg:R，收到了msg:A之后，在一个期待的时间内，如果没有收到ack:N 可以认为是发送超时或者发送失败了
这里有两种做法：
做法一: 上层做重发,例如可以失败后第一次10秒重发,再失败30秒,再失败60秒,再失败提示用户。
做法二: 失败直接提示用户,让用户主动去重发。
这边采用的时第二种。
 无论哪种做法,只是产品设计的不同,都需要做去重处理。
 可能ClientA同时发出了很多消息，故ClientA需要在本地维护一个等待ack队列，并配合timer超时机制，来记录哪些消息没有收到ack:N，以定时重发。
 一旦收到了ack:N，说明ClientB收到了消息，对应的消息将从“等待ack队列”中移除。
### 3. 消息去重
每个服务器自己维护一个自增长ID,`private AtomicLong messageID = new AtomicLong();`
服务器只保存收到的最后一条消息的id,客户端再来消息后,只需要判断来自客户端的id是否大于服务器保存的这个就可以。如果小于则说明是重复消息,剔除即可。
如:当前服务器上的ID为10,客户端连续发送3条消息,ID分别为11,12,13。3条消息都发送到服务器了,服务器上的ID变成了13。
由于网络原因,客户端认为其中某个消息例如12没发成功,又发送过来了,服务器收到消息后判断ID为12由于比当前服务器ID13小,判断为重复消息,就被丢弃了。
### 4. 如何保证消息时序性
#### 原因
1.服务器时间不一致导致,服务器时区问题导致各个服务器上的时间不一致。
2.网络传输导致 由于网络原因可能后发的消息会比先发的消息早发出去
3.服务集群时差导致 由于IM服务器分布式部署，不同的消息可能路由到不同的逻辑层处理。路由到不同logic的时延不同（尤其是跨机房），且不同logic之间存在微量时差。
4.消息处理速度不一致导致 服务器收到消息后，不同logic，不同线程对消息的处理速度可能不同，导致投递消息的时序出现错乱。

#### 解决
1.时间同步 确保服务器端各个服务器之间通过NTP协议实现时间同步，确保各个操作系统时区一致。
2.单聊时序 每条消息增加seq序号,ClientA发送消息时，确保每条消息的seq号递增。服务器推送消息给ClientB时,可能因为网络原因(1.服务器发送时就乱了2.ClientB接收时才乱)再次出现时序错乱。ClientB需要根据seq对消息显示时序进行修正。
3.群聊消息 群聊不能再利用发送方的seq来保证时序，因为发送方不单点，时间也不一致。群聊消息以服务器收到发送消息的顺序为准，服务器为每条消息生成时间有序的msgid，客户端以msgid大小顺序来排序即可。
### 5. 离线消息
#### 简介
如果ClientB不在线,IM-Server保存了离线消息后(保存在MQ中)，要伪造ack:N发送给ClientA；
离线消息的拉取，为了保证消息的可靠性，也需要有ack机制，但由于拉取离线消息不存在N报文，故实际情况要简单的多，即先发送offline:R报文拉取消息，
收到offline:A后，再发送offlineack:R删除离线消息。
#### 初级方案
接收方B要拉取发送方A给ta发送的离线消息，只需在receiver_uid（即接收方B的用户ID), sender_uid（即发送方A的用户ID）上查询，然后把离线消息删除，再把消息返回B即可。

离线消息的拉取，如果用SQL语句来描述的话，它可以是：
```mysql
SELECT msg_id, send_time, msg_type, msg_content 
FROM offline_msgs
WHERE receiver_uid = ? and sender_uid = ?
```

 离线拉取的整体流程如下所示：

Stelp 1：用户B开始拉取用户A发送给ta的离线消息；
Stelp 2：服务器从DB（或对应的持久化容器）中拉取离线消息；
Stelp 3：服务器返回给用户B想要的离线消息
Stelp 4：用户收到消息后服务器从DB（或对应的持久化容器）中把离线消息删除；
#### 优化方案
如果用户B有很多好友，登陆时客户端需要对所有好友进行离线消息拉取，客户端与服务器交互次数就会比较多。
② 优化方案1：
先拉取各个好友的离线消息数量，真正用户B进去看离线消息时，才往服务器发送拉取请求（手机端为了节省流量，经常会使用这个按需拉取的优化）。
③ 优化方案2：
如下图所示，一次性拉取所有好友发送给用户B的离线消息，到客户端本地再根据sender_uid进行计算，这样的话，离校消息表的访问模式就变为->只需要按照receiver_uid来查询了。
登录时与服务器的交互次数降低为了1次。

一般采用方案2，因为移动网络的不可靠性加上电量、流量等资源的昂贵性，能尽量一次性干完的事，就尽可能一次搞定，从而提供整个APP的用户体验。
### 大量离线消息导致速度慢、卡顿
用户B一次性拉取所有好友发给ta的离线消息，消息量很大时，一个请求包很大、速度慢，容易卡顿怎么办？
我们可以分页拉取：根据业务需求，先拉取最新的一页消息，再按需一页页拉取，这样便能很好地解决用户体验问题。


#### 解决重复拉取离线消息的问题
如果用户B拉取了一页离线消息，却在ACK之前crash了，下次登录时会拉取到重复的离线消息么？

确实，拉取了离线消息却没有ACK，服务器不会删除之前的离线消息，故下次登录时系统层面还会拉取到。但在业务层面，可以根据msg_id去重。
SMC理论：系统层面无法做到消息不丢不重，业务层面可以做到，对用户无感知。

解决: 在业务层面，可以根据msg_id去重。
假设有N页离线消息，现在每页离线消息需要一个ACK，那么岂不是客户端与服务器的交互次数又加倍了？
优化:不用每页消息都返回一个ACK,拉去后一页消息就可以看做是前一页消息的ACK,这样只需要最后一页消息返回ACK即可。

## 流程

![](https://ws1.sinaimg.cn/large/006tNbRwly1fylfxevl2ij30it0etaau.jpg)

- 客户端向 `route` 发起登录。
- 登录成功从 `Zookeeper` 中选择可用 `IM-server` 返回给客户端，并保存登录、路由信息到 `Redis`。
- 客户端向 `IM-server` 发起长连接，成功后保持心跳。
- 客户端下线时通过 `route` 清除状态信息。


## 注册中心
### 1. 最初方案
最开始想的是使用Redis来存储服务节点信息。每个节点启动之后都向 Redis 注册信息，关闭时也删除数据。
其实就是存放节点的 `ip + port`，然后在需要知道服务节点信息时候只需要去 Redis 中获取即可。
但这样会导致每次使用时都需要频繁的去查询 Redis，我可以在每次查询之后在本地缓存一份最新的数据。这样优先从本地获取确实可以提高效率。
但同样又会出现新的问题，如果服务提供者的节点新增或者删除消费者这边根本就不知道情况。
要解决这个问题最先想到的应该就是利用定时任务定期去更新服务列表。
以上的方案肯定不完美，并且不优雅。主要有以下几点：

* 基于定时任务会导致很多无效的更新。
* 定时任务存在周期性，没法做到实时，这样就可能存在请求异常。
* 如果服务被强行 kill，没法及时清除 Redis，这样这个看似可用的服务将永远不可用！
所以我们需要一个更加靠谱的解决方案，这样的场景其实和 Dubbo 非常类似。

### 2. 使用ZooKeeper作为注册中心
dubbo官方也是推荐使用ZooKeeper作为注册中心。
Zookeeper 是一个典型的观察者模式。
使用ZooKeeper的临时节点。
IM-Server启动时在/root节点下创建一个`ip+prot`的临时节点。
消费者可以订阅临时节点的父节点`root节点`,利用watcher机制,当新增、删除节点时消费者会收到通知。
同时IM-Server down掉后临时节点也会自动删除。

这样我们就可以实时获取服务节点的信息，同时也只需要在第一次获取列表时缓存到本地；
也不需要频繁和 Zookeeper 产生交互，只用等待通知更新即可。


