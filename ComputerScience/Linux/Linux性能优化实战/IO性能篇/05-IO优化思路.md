# Linux I/O优化思路

## 1. I/O 基准测试

为了更客观合理地评估优化效果，我们首先应该对磁盘和文件系统进行基准测试，得到文件系统或者磁盘 I/O 的极限性能。

fio（Flexible I/O Tester）正是最常用的文件系统和磁盘 I/O 性能基准测试工具。

它提供了大量的可定制化选项，可以用来测试，裸盘或者文件系统在各种场景下的 I/O 性能，包括了不同块大小、不同 I/O 引擎以及是否使用缓存等场景。

```sh
# Ubuntu
apt-get install -y fio

# CentOS
yum install -y fio 
```

安装完成后，就可以执行 man fio 查询它的使用方法。



常见场景测试：

```sh
# 随机读
fio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb

# 随机写
fio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb

# 顺序读
fio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb

# 顺序写
fio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb 
```

在这其中，有几个参数需要你重点关注一下：

* **direct**，表示是否跳过系统缓存。1 表示跳过系统缓存。
* **iodepth**，表示使用异步 I/O（asynchronous I/O，简称 AIO）时，同时发出的 I/O 请求上限。
* **rw**，表示 I/O 模式。 read/write 分别表示顺序读 / 写，而 randread/randwrite 则分别表示随机读 / 写。
* **ioengine**，表示 I/O 引擎，它支持同步（sync）、异步（libaio）、内存映射（mmap）、网络（net）等各种 I/O 引擎。上面示例中设置的 libaio 表示使用异步 I/O。
* **bs**，表示 I/O 的大小。示例中设置的是 4K（这也是默认值）。
* **filename**，表示文件路径，当然，它可以是磁盘路径（测试磁盘性能），也可以是文件路径（测试文件系统性能）。示例中设置的是磁盘 /dev/sdb。**注意：用磁盘路径测试写，会破坏这个磁盘中的文件系统，所以在使用前，你一定要事先做好数据备份**。





## 2. I/O 性能优化

### 1. 应用程序优化

应用程序处于整个 I/O 栈的最上端，它可以通过系统调用，来调整 I/O 模式（如顺序还是随机、同步还是异步）， 同时，它也是 I/O 数据的最终来源。



* 1）**用追加写代替随机写**，减少寻址开销，加快 I/O 写的速度。
* 2）**借助缓存 I/O** ，充分利用系统缓存，降低实际 I/O 的次数。
* 3）在应用程序内部**构建自己的缓存**，或者用 Redis 这类外部缓存系统。
* 4）在需要频繁读写同一块磁盘空间时，可以**用 mmap 代替 read/write**，减少内存的拷贝次数。
* 5）在需要同步写的场景中，**尽量将写请求合并**，而不是让每个请求都同步写入磁盘，即可以用 fsync() 取代 O_SYNC。
* 6）在多个应用程序共享相同磁盘时，为了保证 I/O 不被某个应用完全占用，推荐**使用 cgroups 的 I/O 子系统，来限制进程 / 进程组的 IOPS 以及吞吐量**。
* 7）在使用 CFQ 调度器时，可以用 ionice 来**调整进程的 I/O 调度优先级**，特别是提高核心应用的 I/O 优先级。



###  2. 文件系统优化

应用程序访问普通文件时，实际是由文件系统间接负责，文件在磁盘中的读写。所以，跟文件系统中相关的也有很多优化 I/O 性能的方式。

* 1）根据实际负载场景的不同，选择最适合的文件系统
* 2）选好文件系统后，还可以进一步优化文件系统的配置选项，包括文件系统的特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）等等。
* 3）优化文件系统的缓存
  * 比如优化 pdflush 脏页的刷新频率以及脏页的限额
  * 优化内核回收目录项缓存和索引节点缓存的倾向
* 4）在不需要持久化时，你还可以用内存文件系统  tmpfs，以获得更好的 I/O 性能 。



### 3. 磁盘优化

数据的持久化存储，最终还是要落到具体的物理磁盘中，同时，磁盘也是整个 I/O 栈的最底层。从磁盘角度出发，自然也有很多有效的性能优化方法。

* 1）换用性能更好的磁盘，比如用 SSD 替代 HDD
* 2）使用 RAID ，把多块磁盘组合成一个逻辑磁盘，构成冗余独立磁盘阵列。这样做既可以提高数据的可靠性，又可以提升数据的访问性能。
* 3）针对磁盘和应用程序 I/O 模式的特征，我们可以选择最适合的 I/O 调度算法。比方说，SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法。而数据库应用，我更推荐使用 deadline 算法。
* 4）对应用程序的数据，进行磁盘级别的隔离。比如，我们可以为日志、数据库等 I/O 压力比较重的应用，配置单独的磁盘。
* 5）在顺序读比较多的场景中，我们可以增大磁盘的预读数据。
* 6）优化内核块设备 I/O 的选项。比如，可以调整磁盘队列的长度 /sys/block/sdb/queue/nr_requests，适当增大队列长度，可以提升磁盘的吞吐量（当然也会导致 I/O 延迟增大）。

